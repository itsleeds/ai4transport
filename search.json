[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The course will run from 10:00 to 17:00, with a 1-hour break for lunch.\n\n\n\n\n\n\n\n\nTime\nSession\n\n\n\n\n\n10:00 - 11:30\nSession 1: Foundations of AI in Transport\n\n\n\n11:30 - 13:00\nSession 2: (AI-Powered) Coding and Development\n\n\n\n13:00 - 14:00\nLunch\n\n\n\n14:00 - 15:30\nSession 3: Using LLMs for Reporting and Analysis\n\n\n\n15:30 - 17:00\nSession 4: AI for Transport Data Analysis",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#course-schedule",
    "href": "schedule.html#course-schedule",
    "title": "Schedule",
    "section": "",
    "text": "The course will run from 10:00 to 17:00, with a 1-hour break for lunch.\n\n\n\n\n\n\n\n\nTime\nSession\n\n\n\n\n\n10:00 - 11:30\nSession 1: Foundations of AI in Transport\n\n\n\n11:30 - 13:00\nSession 2: (AI-Powered) Coding and Development\n\n\n\n13:00 - 14:00\nLunch\n\n\n\n14:00 - 15:30\nSession 3: Using LLMs for Reporting and Analysis\n\n\n\n15:30 - 17:00\nSession 4: AI for Transport Data Analysis",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "questionnaire.html",
    "href": "questionnaire.html",
    "title": "Pre-Course Questionnaire",
    "section": "",
    "text": "This questionnaire will help us tailor the ‘AI for Transport’ course to your needs. Please complete the prerequisites before filling out this form."
  },
  {
    "objectID": "questionnaire.html#prerequisites-confirmation",
    "href": "questionnaire.html#prerequisites-confirmation",
    "title": "Pre-Course Questionnaire",
    "section": "Prerequisites Confirmation",
    "text": "Prerequisites Confirmation\n1. I confirm that I have read the course prerequisites and completed the required setup steps.\n\n( ) Yes\n( ) No"
  },
  {
    "objectID": "questionnaire.html#your-details",
    "href": "questionnaire.html#your-details",
    "title": "Pre-Course Questionnaire",
    "section": "Your Details",
    "text": "Your Details\n2. What is your GitHub username?\nShort text answer"
  },
  {
    "objectID": "questionnaire.html#your-setup",
    "href": "questionnaire.html#your-setup",
    "title": "Pre-Course Questionnaire",
    "section": "Your Setup",
    "text": "Your Setup\n3. What is your primary development environment?\n\n( ) VS Code\n( ) RStudio\n( ) Jupyter Notebook\n( ) Other (please specify)\n( ) None\n\n4. Which programming languages are you most comfortable with for data analysis? (Select all that apply)\n\nPython\nR\nSQL\nNone\nOther (please specify)\n\n5. How do you plan to follow the course materials?\n\n( ) On my own computer using a local setup.\n( ) In the cloud using GitHub Codespaces.\n( ) I’m not sure yet.\n\n6. Did the test code in the prerequisites.qmd file run successfully on your machine?\n\n( ) Yes\n( ) No\n( ) I did not run the code"
  },
  {
    "objectID": "questionnaire.html#current-work-and-ai-experience",
    "href": "questionnaire.html#current-work-and-ai-experience",
    "title": "Pre-Course Questionnaire",
    "section": "Current Work and AI Experience",
    "text": "Current Work and AI Experience\n7. Which software or digital tools do you use most often in your day-to-day transport planning work?\nLong text answer\n8. Briefly describe a typical work task you complete on a day-to-day basis.\nLong text answer\n9. What are the most repetitive or boring parts of your work?\nLong text answer\n10. Which parts of your work do you think are most suitable for automation?\nLong text answer\n11. Which AI tools have you heard of or tested? (Select all that apply)\n\nChatGPT\nGitHub Copilot\nMicrosoft 365 Copilot\nGoogle Gemini\nNone\nOther (please specify)\n\n12. Which AI tools, if any, do you currently use in your work?\nLong text answer"
  },
  {
    "objectID": "questionnaire.html#course-expectations",
    "href": "questionnaire.html#course-expectations",
    "title": "Pre-Course Questionnaire",
    "section": "Course Expectations",
    "text": "Course Expectations\n13. What specific skills or knowledge do you hope to gain from this course?\nLong text answer\n14. On a scale of 1 to 5, how would you rate your confidence in using programming (e.g., Python or R) for data analysis?\n\n\nNot confident at all\n\n\n\n\n\n\n\n\n\n\n\nVery confident"
  },
  {
    "objectID": "s2.html",
    "href": "s2.html",
    "title": "Session 2: (AI-Powered) Coding and Development",
    "section": "",
    "text": "This session focuses on how AI can enhance your coding and development workflow. We will cover how to use AI coding assistants and integrate them with version control best practices using Git and GitHub.",
    "crumbs": [
      "Sessions",
      "Session 2: AI-Powered Development"
    ]
  },
  {
    "objectID": "s2.html#introduction",
    "href": "s2.html#introduction",
    "title": "Session 2: (AI-Powered) Coding and Development",
    "section": "",
    "text": "This session focuses on how AI can enhance your coding and development workflow. We will cover how to use AI coding assistants and integrate them with version control best practices using Git and GitHub.",
    "crumbs": [
      "Sessions",
      "Session 2: AI-Powered Development"
    ]
  },
  {
    "objectID": "s2.html#ai-powered-ides",
    "href": "s2.html#ai-powered-ides",
    "title": "Session 2: (AI-Powered) Coding and Development",
    "section": "2 AI-Powered IDEs",
    "text": "2 AI-Powered IDEs\n\nIntroduction to AI Coding Assistants: We’ll look at tools like GitHub Copilot.\nLive Demonstration: An instructor will show an efficient AI-assisted workflow in VS Code, covering:\n\nGenerating code with prompts.\nExplaining complex code blocks.\nDebugging with AI assistance.",
    "crumbs": [
      "Sessions",
      "Session 2: AI-Powered Development"
    ]
  },
  {
    "objectID": "s2.html#guided-practice-ai-assisted-data-cleaning",
    "href": "s2.html#guided-practice-ai-assisted-data-cleaning",
    "title": "Session 2: (AI-Powered) Coding and Development",
    "section": "3 Guided Practice: AI-Assisted Data Cleaning",
    "text": "3 Guided Practice: AI-Assisted Data Cleaning\n\nFollow-along exercise: We will use Copilot to clean a sample transport dataset.\nCollaborative Debugging: We’ll intentionally introduce a bug and use AI tools to find and fix it as a group.",
    "crumbs": [
      "Sessions",
      "Session 2: AI-Powered Development"
    ]
  },
  {
    "objectID": "s2.html#independent-challenge-automating-a-script",
    "href": "s2.html#independent-challenge-automating-a-script",
    "title": "Session 2: (AI-Powered) Coding and Development",
    "section": "4 Independent Challenge: Automating a Script",
    "text": "4 Independent Challenge: Automating a Script\n\nTask: Use Copilot to write a Python script that automates a simple data handling task or generates a plot.\nInstructors will be available to help.",
    "crumbs": [
      "Sessions",
      "Session 2: AI-Powered Development"
    ]
  },
  {
    "objectID": "s2.html#version-control-with-git-and-github",
    "href": "s2.html#version-control-with-git-and-github",
    "title": "Session 2: (AI-Powered) Coding and Development",
    "section": "5 Version Control with Git and GitHub",
    "text": "5 Version Control with Git and GitHub\nAI-powered development is most effective when combined with strong version control habits.\n\nGit Basics: Committing your AI-generated code.\nGitHub Collaboration:\n\nUsing issues to track tasks.\nCreating branches for new features (e.g., gh issue develop 11).\nMaking pull requests to merge your work (gh pr create).",
    "crumbs": [
      "Sessions",
      "Session 2: AI-Powered Development"
    ]
  },
  {
    "objectID": "s2.html#exercise",
    "href": "s2.html#exercise",
    "title": "Session 2: (AI-Powered) Coding and Development",
    "section": "6 Exercise",
    "text": "6 Exercise\nContribute to a shared repository using an AI-assisted workflow.\n\nFork and Clone the exercise repository.\nCreate an issue describing a new feature you will add.\nCreate a branch linked to your issue.\nUse Copilot to write the code for the new feature.\nCommit and push your changes.\nCreate a pull request for your work to be reviewed.",
    "crumbs": [
      "Sessions",
      "Session 2: AI-Powered Development"
    ]
  },
  {
    "objectID": "s4.html",
    "href": "s4.html",
    "title": "Session 4: AI for Transport Data Analysis",
    "section": "",
    "text": "In this session, we’ll explore how AI can accelerate the analysis of transport data. We will focus on a case study using spatio-temporal origin-destination (OD) data.",
    "crumbs": [
      "Sessions",
      "Session 4: Data Analysis"
    ]
  },
  {
    "objectID": "s4.html#introduction",
    "href": "s4.html#introduction",
    "title": "Session 4: AI for Transport Data Analysis",
    "section": "",
    "text": "In this session, we’ll explore how AI can accelerate the analysis of transport data. We will focus on a case study using spatio-temporal origin-destination (OD) data.",
    "crumbs": [
      "Sessions",
      "Session 4: Data Analysis"
    ]
  },
  {
    "objectID": "s4.html#lecture-ai-in-data-analysis",
    "href": "s4.html#lecture-ai-in-data-analysis",
    "title": "Session 4: AI for Transport Data Analysis",
    "section": "2 Lecture: AI in Data Analysis",
    "text": "2 Lecture: AI in Data Analysis\n\nAccelerating the Workflow: How AI helps with data cleaning, preprocessing, and exploratory data analysis (EDA).\nPattern Identification: Using AI to uncover patterns and generate hypotheses from complex transport datasets.",
    "crumbs": [
      "Sessions",
      "Session 4: Data Analysis"
    ]
  },
  {
    "objectID": "s4.html#practical-analyzing-spatio-temporal-od-data-with-ai",
    "href": "s4.html#practical-analyzing-spatio-temporal-od-data-with-ai",
    "title": "Session 4: AI for Transport Data Analysis",
    "section": "3 Practical: Analyzing Spatio-Temporal OD Data with AI",
    "text": "3 Practical: Analyzing Spatio-Temporal OD Data with AI\nWe will work through a guided case study using open access Call Detail Records (CDR) data from Spain.\n\n3.1 1. Data Acquisition and Preparation with AI Assistance\n\nWe will use the spanishoddata package to download OD data.\nUse an AI assistant to help write the R code to filter the data for a specific city (e.g., Seville).\nPrepare the location and flow data for visualization.\n\n\n\n3.2 2. AI-Powered Visualization\n\nUse the flowmapblue package to create an interactive flow map.\nAsk an AI assistant to help customize the map, for example, by adding animation or clustering.\n\n\n\n3.3 3. Interpretation and Reporting\n\nUse an LLM to help interpret the patterns shown in the flow map.\nGenerate a summary of the findings for a report.",
    "crumbs": [
      "Sessions",
      "Session 4: Data Analysis"
    ]
  },
  {
    "objectID": "s4.html#course-wrap-up",
    "href": "s4.html#course-wrap-up",
    "title": "Session 4: AI for Transport Data Analysis",
    "section": "4 Course Wrap-up",
    "text": "4 Course Wrap-up\n\nRecap of the key concepts and skills covered in the course.\nQ&A session.",
    "crumbs": [
      "Sessions",
      "Session 4: Data Analysis"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI for Transport",
    "section": "",
    "text": "This is the source repository for the AI for Transport course website.\nVisit https://itsleeds.github.io/ai4transport/ to see the rendered website.\n\n\nThis course provides a comprehensive introduction to the field of Data Science and AI for Transport.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "AI for Transport",
    "section": "",
    "text": "This course provides a comprehensive introduction to the field of Data Science and AI for Transport.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "s1.html",
    "href": "s1.html",
    "title": "Session 1: Foundations of AI in Transport",
    "section": "",
    "text": "This session provides a foundation for understanding and applying AI in transport planning. We will demystify AI, introduce Data Science as a framework for its effective application, and explore real-world use cases.",
    "crumbs": [
      "Sessions",
      "Session 1: Foundations"
    ]
  },
  {
    "objectID": "s1.html#introduction",
    "href": "s1.html#introduction",
    "title": "Session 1: Foundations of AI in Transport",
    "section": "",
    "text": "This session provides a foundation for understanding and applying AI in transport planning. We will demystify AI, introduce Data Science as a framework for its effective application, and explore real-world use cases.",
    "crumbs": [
      "Sessions",
      "Session 1: Foundations"
    ]
  },
  {
    "objectID": "s1.html#lecture",
    "href": "s1.html#lecture",
    "title": "Session 1: Foundations of AI in Transport",
    "section": "2 Lecture",
    "text": "2 Lecture\n\nDemystifying AI: Key concepts and terminology, with reference to Anthropic’s AI Fluency course.\nData Science Framework: How to apply AI effectively within a robust data science workflow.\nAI in Transport: Overview of applications in transport planning.\nEthical Considerations: “Garbage in, garbage out” vs. prompt and context engineering.",
    "crumbs": [
      "Sessions",
      "Session 1: Foundations"
    ]
  },
  {
    "objectID": "s1.html#practical",
    "href": "s1.html#practical",
    "title": "Session 1: Foundations of AI in Transport",
    "section": "3 Practical",
    "text": "3 Practical\nThis is an interactive session to explore real-world AI use cases in transport.\n\nIdentify AI Opportunities (20 mins):\n\nOn paper, draw a flowchart of a typical task from your work.\nDraw a second flowchart showing how AI could be integrated to improve this workflow.\nBonus: If you have time, repeat for another task. Which is more suitable for AI integration?\n\nGroup Discussion (20 mins):\n\nIn small groups, discuss how AI could be best integrated into your work from first principles.\n\nGroup Feedback (20 mins):\n\nShare key themes and ideas from your group discussion with everyone.",
    "crumbs": [
      "Sessions",
      "Session 1: Foundations"
    ]
  },
  {
    "objectID": "s1.html#next-steps",
    "href": "s1.html#next-steps",
    "title": "Session 1: Foundations of AI in Transport",
    "section": "4 Next steps",
    "text": "4 Next steps\nThe course team will consider the following next steps:\n\nDraft and share survey for potential attendees\nDraft and share course outline for feedback\nDraft Sections 1 and 4 (Robin)\nDraft Sections 2 and 3 (Chris)\nReview and finalise content (both)\nConvert this into website and hosting (Robin, Netlify or GitHub Pages)",
    "crumbs": [
      "Sessions",
      "Session 1: Foundations"
    ]
  },
  {
    "objectID": "tfse.html",
    "href": "tfse.html",
    "title": "TfSE AI for Transport Course",
    "section": "",
    "text": "We’re developing materials to support teaching of AI for transport, building on introductory webinars on AI and transport that will cover:\n\nWebinar 1: What is AI?\nUse of AI in data analysis\nApplication of AI in transport planning"
  },
  {
    "objectID": "tfse.html#current-working-practices",
    "href": "tfse.html#current-working-practices",
    "title": "TfSE AI for Transport Course",
    "section": "3.1 Current working practices",
    "text": "3.1 Current working practices\n\nWhich software, web applications or other digital tools do you use for your day-to-day transport planning work?\nWith reference to a recent project or concrete example, describe typical work tasks that you need to complete on a day-to-day basis.\nWhat are the most boring parts of your work?\nWhich parts of your work are most conducive to automation?"
  },
  {
    "objectID": "tfse.html#ai-tools",
    "href": "tfse.html#ai-tools",
    "title": "TfSE AI for Transport Course",
    "section": "3.2 AI tools",
    "text": "3.2 AI tools\n\nWhich AI tools have you heard of?\nWhich AI tools have you tested?\nWhich AI tools do you currently use in your work (if any)?\nAre there any particular AI tools or technologies you are interested in learning more about?"
  },
  {
    "objectID": "tfse.html#data-and-data-science",
    "href": "tfse.html#data-and-data-science",
    "title": "TfSE AI for Transport Course",
    "section": "3.3 Data and data science",
    "text": "3.3 Data and data science\n\nHow do you currently use data in your decision-making processes?\nWhat kinds of data do you use?\nWhich data science tools do you use?\nDo you use any programming languages (e.g., Python, R) for data analysis?\nDo you use interactive development environments (e.g. VS Code) for your work, and if so which?"
  },
  {
    "objectID": "tfse.html#general",
    "href": "tfse.html#general",
    "title": "TfSE AI for Transport Course",
    "section": "3.4 General",
    "text": "3.4 General\n\nWhat skills or knowledge do you hope to gain from this course?"
  },
  {
    "objectID": "tfse.html#course-overview",
    "href": "tfse.html#course-overview",
    "title": "TfSE AI for Transport Course",
    "section": "3.5 Course Overview",
    "text": "3.5 Course Overview\nThis 6-hour course provides a comprehensive introduction to the field of Data Science and AI for Transport.\nWe have found consistently that the best way to take advantage of the new AI tools and techniques is to integrate them into robust and reproducible data science methods. The principles of “garbage in, garbage out” and “all models are wrong but some are useful” applies equally to AI models as they do to traditional models. With the power of AI, it is even more important to have have high-quality input datasets, reproducible workflows that enable quality assurance (QA) steps and agile development practices that can adapt to changing datasets, requirements and computational capabilities.\nThis course will focus on practical applications and real-world examples to help participants understand how to leverage AI in their work.\nParticipants will consolidate their knowledge of fundamental concepts, tools, and techniques and learn to apply them to analyze transport datasets, gain insights, and add value. By the end of it, you will be empowered to use AI and other digital tools to convert raw datasets into actionable evidence for safer, healthier and more efficient transportation systems. The course will be tailored to the specific needs and backgrounds of Transport for the South East."
  },
  {
    "objectID": "tfse.html#syllabus-6-hours-total",
    "href": "tfse.html#syllabus-6-hours-total",
    "title": "TfSE AI for Transport Course",
    "section": "3.6 Syllabus (6 hours total)",
    "text": "3.6 Syllabus (6 hours total)\nThe course will run from 10:00 to 17:00, with a 1-hour break for lunch.\n\n3.6.1 Module 1: Foundations of AI in Transport (10:00 - 11:30)\n\nLecture (30 mins):\n\nDemystifying AI: Key concepts and terminology.\n\nCross-referencing Anthropic’s free AI Fluency course\n\nData Science as a framework for applying AI effectively.\n\nOur own experience and unique take on it\n\nOverview of AI applications in transport planning.\n\n…\n\n3.7 Ethical considerations and “garbage in, garbage out” vs prompt engineering and context engineering.\n\nPractical (60 mins):\n\nInteractive session exploring real-world AI use cases in transport.\n\nHands-on exercise identifying potential AI applications in common transport planning tasks (~20 mins).\n\nStart with pen and paper (recommended)\nDraw a flowchart of an example from your current work\nDraw a flowchart that shows how AI could be introduced\nBonus if fast: do for another project/workflow, which project is most conducive to AI integration?\n\nIn groups of around 5 people: talk about how best to integrate AI in your work from first principles (~20 mins).\nFeedback to everyone and discuss common themes (~20 mins).\n\n\n\n3.7.1 Module 2: (AI-Powered) Coding and Development (11:30 - 13:00)\n\n\n\n\nI Do: Live Demonstration (11:30 - 12:00)\n\nInstructor introduces AI-powered IDEs (VS Code) and AI coding assistants (GitHub Copilot).\nDemonstrates an efficient AI-assisted workflow, highlighting best practices for prompting and when to use AI.\nLive coding: Using Copilot to generate, explain, and debug Python code for a common transport data task (e.g., loading a dataset).\n\nWe Do: Guided Practice (12:00 - 12:30)\n\nBuilding on the “I Do” example, participants follow along with the instructor.\nGuided exercise: Use Copilot to perform the next step, such as cleaning the data or adding a new feature.\nCollaborative problem-solving: The instructor introduces a bug, and the group uses AI tools to identify and fix it together.\n\nYou Do: Independent Application (12:30 - 13:00)\n\nParticipants receive a small, related challenge to solve independently or in pairs.\nTask: Use Copilot to automate a script for data handling or generate a simple visualization.\nInstructor provides support and reviews solutions.\n\n\n# Make a graph showing x and x^2:\nimport matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 10, 100)\ny = x**2\nplt.plot(x, y)   \n\n\n3.7.2 Lunch Break (13:00 - 14:00)\n\n\n3.7.3 Module 3: Using LLMs for Reporting and Analysis (14:00 - 15:30)\n\nLecture (30 mins):\n\nIntroduction to Large Language Models (LLMs) and prompt engineering.\nTechniques for using LLMs with transport-related documents and data.\n\nPractical (60 mins):\n\nReading and summarising transport policy documents.\nReal-world example: analysing 100+ Local Cycling and Walking Infrastructure Plans.\nAnalysing qualitative data (e.g., consultation responses).\nDrafting sections of a transport report based on provided data.\n\n\n\n\n3.7.4 Module 4: AI for Transport Data Analysis (15:30 - 17:00)\n\nLecture (15 mins):\n\nHow AI accelerates data cleaning, preprocessing, and exploratory analysis.\nUsing AI to identify patterns and generate hypotheses from transport datasets.\n\nPractical (75 mins):\n\nGuided case study: Using AI tools to analyze a transport dataset from start to finish.\nGenerating visualizations and interpreting results with AI assistance.\nCourse wrap-up and Q&A."
  },
  {
    "objectID": "tfse.html#resourcing",
    "href": "tfse.html#resourcing",
    "title": "TfSE AI for Transport Course",
    "section": "3.8 Resourcing",
    "text": "3.8 Resourcing\nThe development and initial delivery of this course is estimated at 5 days total, including preparation and delivery:\n\nRobin Lovelace: 3 days\nChris: 2 days"
  },
  {
    "objectID": "s3.html",
    "href": "s3.html",
    "title": "Session 3: Automating Document Analysis with LLM APIs",
    "section": "",
    "text": "Building on Session 2’s AI-powered coding, this session shows how to call an LLM API from code to automate repeatable analysis tasks. You’ll set up a client securely, make simple requests, request structured (JSON) outputs, and analyse a PDF fetched from the web. This sets you up for Session 4, where we apply AI to transport data analysis.",
    "crumbs": [
      "Sessions",
      "Session 3: LLMs for Reporting"
    ]
  },
  {
    "objectID": "s3.html#introduction",
    "href": "s3.html#introduction",
    "title": "Session 3: Automating Document Analysis with LLM APIs",
    "section": "",
    "text": "Building on Session 2’s AI-powered coding, this session shows how to call an LLM API from code to automate repeatable analysis tasks. You’ll set up a client securely, make simple requests, request structured (JSON) outputs, and analyse a PDF fetched from the web. This sets you up for Session 4, where we apply AI to transport data analysis.",
    "crumbs": [
      "Sessions",
      "Session 3: LLMs for Reporting"
    ]
  },
  {
    "objectID": "s3.html#what-youll-learn",
    "href": "s3.html#what-youll-learn",
    "title": "Session 3: Automating Document Analysis with LLM APIs",
    "section": "2 What you’ll learn",
    "text": "2 What you’ll learn\n\nSet up an API client\nSend a basic prompt and parse a response\nAsk the model for structured JSON and validate it\nFetch a PDF from a URL, extract text, and summarise it reproducibly\nBatch a task across multiple files",
    "crumbs": [
      "Sessions",
      "Session 3: LLMs for Reporting"
    ]
  },
  {
    "objectID": "s3.html#setup",
    "href": "s3.html#setup",
    "title": "Session 3: Automating Document Analysis with LLM APIs",
    "section": "3 Setup",
    "text": "3 Setup\nWe’ll use Python. If packages are missing, install them first.\n\n#!pip install openai requests pypdf\nimport os\nimport json\nfrom typing import List, Dict\nfrom openai import OpenAI\n\nBest practice is to set your API key as an environment variable (don’t hard-code secrets). You can do this using PowerShell or through the .env file directly.\nNow create the client:\n\nROUTER_API_KEY = 'link to your api key location'\n\nCLIENT = OpenAI(\n    api_key=ROUTER_API_KEY,\n    default_headers={\"Authorization\": f\"Bearer {ROUTER_API_KEY}\"}\n)",
    "crumbs": [
      "Sessions",
      "Session 3: LLMs for Reporting"
    ]
  },
  {
    "objectID": "s3.html#a-simple-chat-completion",
    "href": "s3.html#a-simple-chat-completion",
    "title": "Session 3: Automating Document Analysis with LLM APIs",
    "section": "4 A simple chat completion",
    "text": "4 A simple chat completion\nThis mirrors what you do in a chat UI, but from code:\n\nresp = CLIENT.chat.completions.create(\n    model=\"gpt-5-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a dilligent, concise transport planning assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me a family-friendly transport planning joke.\"}\n    ]\n)\nprint(resp.choices[0].message.content)\n\nTry tweaking the prompt style (e.g., more witty, drier, aimed at students vs. professionals).",
    "crumbs": [
      "Sessions",
      "Session 3: LLMs for Reporting"
    ]
  },
  {
    "objectID": "s3.html#getting-structured-json-outputs",
    "href": "s3.html#getting-structured-json-outputs",
    "title": "Session 3: Automating Document Analysis with LLM APIs",
    "section": "5 Getting structured (JSON) outputs",
    "text": "5 Getting structured (JSON) outputs\nWhen you need to use the response downstream, ask the model to return JSON and validate it.\n\nschema_hint = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"title\": {\"type\": \"string\"},\n        \"insight\": {\"type\": \"string\"},\n        \"confidence\": {\"type\": \"number\"}\n    },\n    \"required\": [\"title\", \"insight\", \"confidence\"]\n}\n\nprompt = (\n    \"Summarise one noteworthy transport planning insight from London’s congestion pricing \"\n    \"policy in 1-2 sentences. Return only valid JSON with keys: title, insight, confidence (0-1).\"\n)\n\nresp = CLIENT.chat.completions.create(\n    model=\"gpt-5-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"Return only JSON, no extra text.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n)\n\nraw = resp.choices[0].message.content\ntry:\n    data = json.loads(raw)\n    assert set([\"title\", \"insight\", \"confidence\"]) &lt;= set(data.keys())\nexcept Exception as e:\n    raise ValueError(f\"Model did not return valid JSON. Raw output: {raw}\") from e\n\ndata",
    "crumbs": [
      "Sessions",
      "Session 3: LLMs for Reporting"
    ]
  },
  {
    "objectID": "s3.html#analysing-a-pdf-from-a-url-robust-approach",
    "href": "s3.html#analysing-a-pdf-from-a-url-robust-approach",
    "title": "Session 3: Automating Document Analysis with LLM APIs",
    "section": "6 Analysing a PDF from a URL (robust approach)",
    "text": "6 Analysing a PDF from a URL (robust approach)\nRather than relying on vendor-specific file-upload APIs, we’ll fetch the PDF, extract text locally, then send concise excerpts to the model. This is portable and keeps you in control of pre-processing.\n\n!pip install pypdf\nimport io\nimport math\nimport requests\nfrom pypdf import PdfReader\n\ndef fetch_pdf_text(url: str, max_pages: int = 5) -&gt; str:\n    \"\"\"Download a PDF and extract text from the first `max_pages` pages.\"\"\"\n    r = requests.get(url, timeout=30)\n    r.raise_for_status()\n    with io.BytesIO(r.content) as f:\n        reader = PdfReader(f)\n        pages = min(max_pages, len(reader.pages))\n        text = []\n        for i in range(pages):\n            text.append(reader.pages[i].extract_text() or \"\")\n    return \"\\n\\n\".join(text).strip()\n\ndef chunk_text(text: str, chunk_chars: int = 6000) -&gt; List[str]:\n    \"\"\"Split text into roughly token-sized chunks for prompting.\"\"\"\n    chunks = []\n    for i in range(0, len(text), chunk_chars):\n        chunks.append(text[i:i+chunk_chars])\n    return chunks\n\ndef summarise_pdf(url: str, task: str = \"Summarise and comment on this document for a transport audience.\") -&gt; str:\n    text = fetch_pdf_text(url)\n    if not text:\n        return \"No text extracted from the PDF.\"\n    chunks = chunk_text(text)\n    summaries = []\n    for idx, ch in enumerate(chunks, start=1):\n        r = CLIENT.chat.completions.create(\n            model=\"gpt-5-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a precise analyst. Keep outputs concise.\"},\n                {\"role\": \"user\", \"content\": f\"{task}\\n\\nChunk {idx}/{len(chunks)}:\\n{ch}\"}\n            ]\n        )\n        summaries.append(r.choices[0].message.content)\n    # Compress partial summaries into a final synthesis\n    final = CLIENT.chat.completions.create(\n        model=\"gpt-5-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"Create a single, coherent summary with key takeaways.\"},\n            {\"role\": \"user\", \"content\": \"\\n\\n\".join(summaries)}\n        ]\n    )\n    return final.choices[0].message.content\n\n# Example (uses a classic sample PDF)\nsample_url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\nsummarise_pdf(sample_url)\n\n\n6.1 Try it\n\nChange max_pages in fetch_pdf_text to control cost/speed.\nSwap task to focus on risks, methods, or key findings.\nUse your own PDF URLs (public reports, guidance docs, etc.).",
    "crumbs": [
      "Sessions",
      "Session 3: LLMs for Reporting"
    ]
  },
  {
    "objectID": "s3.html#batch-a-task-across-multiple-documents",
    "href": "s3.html#batch-a-task-across-multiple-documents",
    "title": "Session 3: Automating Document Analysis with LLM APIs",
    "section": "7 Batch a task across multiple documents",
    "text": "7 Batch a task across multiple documents\nYou’ll often need a small, consistent record per file (e.g., title, 2–3 bullet insights, and a confidence score). Here’s a simple batcher that writes JSON Lines (one JSON object per line):\n\nfrom datetime import datetime\n\ndef extract_insights(text: str) -&gt; Dict:\n    prompt = (\n        \"From the provided text, output JSON with: title, bullets (array of 2-3 concise points),\"\n        \" confidence (0-1). Return only JSON.\"\n    )\n    r = CLIENT.chat.completions.create(\n        model=MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": \"Return valid JSON only.\"},\n            {\"role\": \"user\", \"content\": f\"{prompt}\\n\\n{text[:6000]}\"}\n        ]\n    )\n    raw = r.choices[0].message.content\n    try:\n        return json.loads(raw)\n    except Exception:\n        return {\"title\": \"(parse_error)\", \"bullets\": [raw], \"confidence\": 0.0}\n\ndef batch_process(urls: List[str], out_path: str = \"insights.jsonl\") -&gt; str:\n    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n        for url in urls:\n            try:\n                text = fetch_pdf_text(url, max_pages=5)\n                data = extract_insights(text)\n                data.update({\"source\": url, \"ts\": datetime.utcnow().isoformat()})\n                f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n            except Exception as e:\n                f.write(json.dumps({\n                    \"source\": url, \"error\": str(e), \"ts\": datetime.utcnow().isoformat()\n                }) + \"\\n\")\n    return out_path\n\nurls = [\n    \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\",\n    # add more public PDF URLs here\n]\n\nbatch_process(urls)",
    "crumbs": [
      "Sessions",
      "Session 3: LLMs for Reporting"
    ]
  },
  {
    "objectID": "s3.html#good-practice-and-guardrails",
    "href": "s3.html#good-practice-and-guardrails",
    "title": "Session 3: Automating Document Analysis with LLM APIs",
    "section": "8 Good practice and guardrails",
    "text": "8 Good practice and guardrails\n\nReproducibility: fix the model name per project and log prompts.\nPrivacy: don’t send sensitive data to third-party APIs without approval.\nCost control: cap pages, chunk sizes, and batch sizes; cache intermediate results.\nError handling: catch network errors/timeouts; write partial results with error notes.\nVersion control: commit your scripts/notebooks; review diffs of prompt changes (links to Session 2).",
    "crumbs": [
      "Sessions",
      "Session 3: LLMs for Reporting"
    ]
  },
  {
    "objectID": "s3.html#where-this-fits-in-the-course",
    "href": "s3.html#where-this-fits-in-the-course",
    "title": "Session 3: Automating Document Analysis with LLM APIs",
    "section": "9 Where this fits in the course",
    "text": "9 Where this fits in the course\n\nFrom Session 1: we’re applying AI within a clear workflow with ethical awareness.\nFrom Session 2: we’re turning assisted coding into reproducible automation.\nInto Session 4: we’ll apply the same ideas to transport data analysis and reporting.",
    "crumbs": [
      "Sessions",
      "Session 3: LLMs for Reporting"
    ]
  },
  {
    "objectID": "slides/day1.html#welcome",
    "href": "slides/day1.html#welcome",
    "title": "AI for Transport",
    "section": "Welcome!",
    "text": "Welcome!\nAI for Transport\n1-day course\nOctober 2025"
  },
  {
    "objectID": "slides/day1.html#agenda",
    "href": "slides/day1.html#agenda",
    "title": "AI for Transport",
    "section": "Agenda",
    "text": "Agenda\n\n10:00-11:30 Session 1: Foundations of AI in Transport\n11:30-13:00 Session 2: (AI-Powered) Coding and Development\n13:00-14:00 Lunch\n14:00-15:30 Session 3: Using LLMs for Reporting and Analysis\n15:30-17:00 Session 4: AI for Transport Data Analysis"
  },
  {
    "objectID": "slides/day1.html#prerequisites",
    "href": "slides/day1.html#prerequisites",
    "title": "AI for Transport",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo participate\n\nComputer with internet access\nA willingness to learn and experiment with AI tools\nBasic familiarity with transport planning concepts\nOptional: Experience with coding or data analysis tools"
  },
  {
    "objectID": "slides/day1.html#learn-and-share",
    "href": "slides/day1.html#learn-and-share",
    "title": "AI for Transport",
    "section": "Learn and share",
    "text": "Learn and share\nThe following will help:\n\nAn interest in transport planning and AI applications\nA willingness to learn and share experiences\nA GitHub account (useful for accessing course materials)\nOpenness to trying new AI tools and workflows"
  },
  {
    "objectID": "slides/day1.html#housekeeping",
    "href": "slides/day1.html#housekeeping",
    "title": "AI for Transport",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nConnect to the Wi-Fi network\nCourse materials available online\nCoffee and lunch breaks as scheduled\nInteractive sessions: Please participate and ask questions"
  },
  {
    "objectID": "slides/day1.html#course-principles",
    "href": "slides/day1.html#course-principles",
    "title": "AI for Transport",
    "section": "Course principles",
    "text": "Course principles\n\n“Learn by doing” with AI tools\n“Demystifying AI” for practical use\n“No such thing as a bad question”\nFocus on practical applications\nReal-world transport examples\nHands-on experience with AI workflows"
  },
  {
    "objectID": "slides/day1.html#about-the-instructors",
    "href": "slides/day1.html#about-the-instructors",
    "title": "AI for Transport",
    "section": "About the instructors",
    "text": "About the instructors\nRobin Lovelace - Professor of Transport Data Science, University of Leeds - Focus on data-driven transport planning and open source tools\nChris Rushton - Transport planning and AI specialist - Experience in practical AI applications"
  },
  {
    "objectID": "slides/day1.html#about-you",
    "href": "slides/day1.html#about-you",
    "title": "AI for Transport",
    "section": "About you",
    "text": "About you\nQuick introductions: - Name and organisation - Current role in transport - Experience with AI tools (if any) - What you hope to gain from today"
  },
  {
    "objectID": "slides/day1.html#session-1-foundations-of-ai-in-transport",
    "href": "slides/day1.html#session-1-foundations-of-ai-in-transport",
    "title": "AI for Transport",
    "section": "Session 1: Foundations of AI in Transport",
    "text": "Session 1: Foundations of AI in Transport\nKey topics:\n\nWhat is AI and how does it apply to transport?\nOverview of AI tools and techniques\nReal-world transport applications\nEthical considerations\n\nSee: Session 1 materials"
  },
  {
    "objectID": "slides/day1.html#session-2-ai-powered-coding-and-development",
    "href": "slides/day1.html#session-2-ai-powered-coding-and-development",
    "title": "AI for Transport",
    "section": "Session 2: (AI-Powered) Coding and Development",
    "text": "Session 2: (AI-Powered) Coding and Development\nKey topics:\n\nAI coding assistants (GitHub Copilot, etc.)\nEfficient AI-assisted workflows\nLive coding demonstrations\nHands-on practice\n\nSee: Session 2 materials"
  },
  {
    "objectID": "slides/day1.html#lunch-break-1300-1400",
    "href": "slides/day1.html#lunch-break-1300-1400",
    "title": "AI for Transport",
    "section": "Lunch Break (13:00-14:00)",
    "text": "Lunch Break (13:00-14:00)"
  },
  {
    "objectID": "slides/day1.html#session-3-using-llms-for-reporting-and-analysis",
    "href": "slides/day1.html#session-3-using-llms-for-reporting-and-analysis",
    "title": "AI for Transport",
    "section": "Session 3: Using LLMs for Reporting and Analysis",
    "text": "Session 3: Using LLMs for Reporting and Analysis\nKey topics:\n\nLarge Language Models for transport documents\nPrompt engineering techniques\nAnalysing policy documents and reports\nPractical exercises with real transport data\n\nSee: Session 3 materials"
  },
  {
    "objectID": "slides/day1.html#session-4-ai-for-transport-data-analysis",
    "href": "slides/day1.html#session-4-ai-for-transport-data-analysis",
    "title": "AI for Transport",
    "section": "Session 4: AI for Transport Data Analysis",
    "text": "Session 4: AI for Transport Data Analysis\nKey topics:\n\nAI-accelerated data processing\nPattern recognition in transport datasets\nVisualisation with AI assistance\nComplete workflow examples\n\nSee: Session 4 materials"
  },
  {
    "objectID": "slides/day1.html#feedback-on-session-4",
    "href": "slides/day1.html#feedback-on-session-4",
    "title": "AI for Transport",
    "section": "Feedback on Session 4",
    "text": "Feedback on Session 4\nPlease provide feedback on this session: forms.office.com/e/XZ2Hdt72HK"
  },
  {
    "objectID": "slides/day1.html#course-wrap-up",
    "href": "slides/day1.html#course-wrap-up",
    "title": "AI for Transport",
    "section": "Course wrap-up",
    "text": "Course wrap-up\nKey takeaways:\n\nAI as a tool to enhance transport planning\nPractical skills for immediate application\nResources for continued learning\nBuilding AI into your workflow\n\nNext steps:\n\nTry the tools in your own work\nJoin the community discussions\nShare your experiences and learn from others"
  },
  {
    "objectID": "slides/day1.html#thank-you",
    "href": "slides/day1.html#thank-you",
    "title": "AI for Transport",
    "section": "Thank you!",
    "text": "Thank you!\nQuestions? Discussion?\nCourse materials: Available online Community: GitHub Discussions"
  },
  {
    "objectID": "slides/day1.html#references",
    "href": "slides/day1.html#references",
    "title": "AI for Transport",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "prerequisites.html",
    "href": "prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "We assume you are familiar with transport datasets and have basic data analysis skills.\nYou must have a GitHub account and have saved your username.\nWe will cover version control concepts in the course.",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "prerequisites.html#course-prerequisites",
    "href": "prerequisites.html#course-prerequisites",
    "title": "Prerequisites",
    "section": "",
    "text": "We assume you are familiar with transport datasets and have basic data analysis skills.\nYou must have a GitHub account and have saved your username.\nWe will cover version control concepts in the course.",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "prerequisites.html#software-prerequisites",
    "href": "prerequisites.html#software-prerequisites",
    "title": "Prerequisites",
    "section": "2 Software Prerequisites",
    "text": "2 Software Prerequisites\nYou should bring a laptop with either of the following:\n\nOption 1: if you’re using a cloud-based development environment:\n\nA modern web browser (e.g., Chrome, Firefox, Edge).\nA GitHub account (sign up at github.com) and have your username ready.\nTested out GitHub Codespaces to ensure it works on your machine by opening this link and running the prerequisites code in prerequisites.qmd:\n\n\n\n\n\nOpen in GitHub Codespaces\n\n\nor, for running the code locally:\n\nOption 2: A laptop with a the necessary software installed, including:\n\nAn IDE such as VS Code (recommended) or RStudio.\nR or Python installed (see below for testing code).\nThe gh command-line tool (see cli.github.com for installation and set-up instructions).",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "prerequisites.html#recommended-online-courses",
    "href": "prerequisites.html#recommended-online-courses",
    "title": "Prerequisites",
    "section": "3 Recommended Online Courses",
    "text": "3 Recommended Online Courses\nTo prepare for this course, we recommend watching the following short video:\n\nIntroduction to AI Fluency by Anthropic (Lesson 1, ~5 minutes).\n\nAnd taking these short but very useful online courses:\n\nIntro to GitHub (should take less than an hour).\nCommunicate using Markdown (should take around 30 minutes or less).",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "prerequisites.html#testing-your-setup",
    "href": "prerequisites.html#testing-your-setup",
    "title": "Prerequisites",
    "section": "4 Testing your setup",
    "text": "4 Testing your setup\nYou can test your setup by running the following code in Python or R, either in your local IDE or in GitHub Codespaces. Do so by first creating a new Quarto (.qmd) file, e.g. called test.qmd, and then typing the following into a code chunk with three backticks (located in the top left of your keyboard, just left of the 1 key) to start and end the code chunk and {r} or {python} at the end of the first code chunk to make the code interactive, as follows (update the code as needed):\nTo run code in .qmd files interactively, ensure your cursor is focused in a code chunk (or you have specific lines of code selected) and then click the “Run Cell” button or (preferably) do it with keyboard shortcuts: press Ctrl+Enter to run the current line or Ctrl+Shift+Enter to run the entire code chunk.  See the Quarto documentation at quarto.org/docs/ for more details on how to run code chunks in Quarto.\n\nYou should be able to run the code in a GitHub Codespaces with the following link:\n\n\n\nOpen in GitHub Codespaces\n\n\nChoose either the Python or R code below, depending on which language you prefer to use.\n\nRPython\n\n\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"pak\")) install.packages(\"pak\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"tmap\",\n    \"osmextract\"\n)\npak::pak(pkgs)\n\n\nlibrary(tidyverse)\nlibrary(sf)\n\nThe next bit optionally downloads lots of data, so you may want to skip it if you’re just testing your setup.\n\n\nCode\n# Centered on Broadway House, London\nbroadway_house = stplanr::geo_code(\"Tothill St, London\")\n# [1] -0.1302077 51.4996820\nstudy_area = st_point(c(-0.13020, 51.4997)) |&gt; \n  st_sfc(crs = 4326) |&gt;\n  st_transform(27700) |&gt;\n  st_buffer(500) |&gt;\n  st_transform(4326)\nextra_tags = c(\n  \"maxspeed\",\n  \"lit\",\n  \"cycleway\"\n)\nosm_network = osmextract::oe_get_network(\n  place = study_area,\n  boundary = study_area,\n  boundary_type = \"clipsrc\",\n  extra_tags = extra_tags,\n  mode = \"driving\"\n)\nsf::write_sf(osm_network, \"osm_network.geojson\", delete_dsn = TRUE)\n\n\n\nu = \"https://github.com/itsleeds/ai4transport/raw/main/osm_network.geojson\"\nosm_network = sf::read_sf(u)\n\nIf the command above fails, see instructions below:\n\nYour computer cannot access the file osm_network.geojson, perhaps due to a firewall or network issue. You can solve this issue as follows:\nManually download the file from here or if GitHub is blocked, you can access it from OneDrive here.\n\n\nlibrary(tmap)\n# Uncomment the next line to view in interactive window:\n# tmap_mode(\"view\") \nm = tm_shape(osm_network) +\n  tm_lines(\"maxspeed\")\nm\n\n\n\n\n\n\n\n\n\n\n\nimport osmnx as ox\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport shapely\n\n# Import and plot the saved data:\ngdf = gpd.read_file(\"osm_network.geojson\")\ngdf.explore(column=\"maxspeed\")\n\n# Optional: Download data from OSM:\nstudy_point = shapely.Point(-0.13020, 51.4997)\nstudy_geom = gpd.GeoSeries([study_point], crs=4326)\nstudy_polygon = study_geom.to_crs(epsg=3857).buffer(500).to_crs(epsg=4326).unary_union\nstudy_polygon_gpd = gpd.GeoDataFrame(geometry=[study_polygon], crs=\"EPSG:4326\")\ntags = {\"highway\": True, \"maxspeed\": True, \"lit\": True, \"cycleway\": True}\ngdf = ox.features_from_polygon(study_polygon, tags)\ngdf = gdf[gdf.geom_type.isin([\"LineString\", \"MultiLineString\"])]\ngdf = gdf.to_crs(epsg=3857)\ngdf.plot(column=\"maxspeed\", figsize=(10, 10), legend=True)\nplt.show()\n\n\n\nIf it worked, it should look something like this (from the online development version):\n\n\nThat is the road network surrounding Broadway House in London, where the course will be held in person.\n\n\nLet us know how you get on and if you have any issues getting set up, either by email, or (preferably) via the Discussion forum on GitHub associated with this course repository.",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "prerequisites.html#next-steps",
    "href": "prerequisites.html#next-steps",
    "title": "Prerequisites",
    "section": "5 Next Steps",
    "text": "5 Next Steps\nEveryone should complete the Pre-Course Questionnaire before the course begins.",
    "crumbs": [
      "Prerequisites"
    ]
  }
]
---
title: "Session 3: AI for Transport Data Analysis"
execute:
    eval: false
---

## Introduction

In this session, we will explore how AI can be used to augment and accelerate data analysis tasks in transport planning. We will focus on using AI to write code for analyzing and visualizing transport data.

## Finding and Cleaning Data with AI

<!-- *Placeholder: Describe how to use AI tools (like ChatGPT or Copilot) to identify relevant datasets and write scripts to clean and format them.* -->

Generally available AI models, such as GPT (currently used in M365 Copilot and the models you can use in GitHub Copilot) are generalists.
That means they are *not* generally good at very specific domains, such as the locations of transport datasets.

::: note
You can check which model you are using by asking the AI directly.

<details>
<summary>Example conversation</summary>

**User**: What model are you?

**Copilot**: I’m based on GPT‑5, a large language model developed by OpenAI and integrated into Microsoft Copilot. This means I can handle a wide range of tasks—answering questions, generating content, analyzing data, and more—while adapting to your context.

</details>
:::

To provide an example, imagine you need a dataset of the motorway network in the South East of England.
You could try this prompt in an AI chat tool, as shown in the details below.

However, a more effective approach is likely to be to ask the AI to help you write code to search for and download relevant datasets from known repositories, such as data.gov.uk or OpenStreetMap.

### Example: Downloading motorway data

1. Write a prompt that results in code to download motorway data in the South East of England.

2. Update the prompt to make it more specific, e.g. telling it to use a specific language or package.

3. Try running the code.

<details>
<summary>Example prompt and output</summary>

**Prompt:** 

> "Write R code to download motorway network data for the South East of England from OpenStreetMap using the `osmdata` package."

**AI Output:**

```{r}
#| eval: false
library(osmdata)
library(sf)
# Define the bounding box for South East England
bbox <- getbb("South East England, UK")
# Create an Overpass query for motorways
motorway_query <- opq(bbox) %>%
  add_osm_feature(key = "highway", value = "motorway")
# Download the data
motorway_data <- osmdata_sf(motorway_query)
# Extract the motorway lines
motorways <- motorway_data$osm_lines
# Plot the result
plot(st_geometry(motorways), col = 'blue', main = "Motorway Network in South East England")
# Save to a shapefile
st_write(motorways, "south_east_motorways.shp")
```

**Notes:**

1. It worked in one shot!

A year ago this would not have worked first time, but the models are improving rapidly.

2. However, it was not perfect:

  - It saved the file in an out-of data file format (shapefile) rather than a more modern one (GeoPackage).
  - It used the outdated `%>%` pipe operator from `magrittr` rather than the native R pipe `|>`.

3. Refining the prompt can easily fix these issues.

</details>

```{r}
#| eval: false
#| echo: false
# Manual updated to save as GeoPackage:
st_write(motorways, "south_east_motorways.gpkg")
system("gh release list")
system("gh release upload v1 south_east_motorways.gpkg")
```

```{r}
#| echo: false
motorways_url = "https://github.com/itsleeds/ai4transport/releases/download/v1/south_east_motorways.gpkg"
motorways = sf::read_sf(motorways_url)
motorways = motorways |>
  # description 	London Orbital Motorway
  subset(description == "London Orbital Motorway")
mapview::mapview(motorways, zcol = "lanes", legend = TRUE)
```

### Bonus activity

Use AI to help you find and import data on a topic of your choice relevant to transport planning.

## Exploratory Data Analysis (EDA) with AI

Take a read of the AI-assisted EDA session in [survey-prompts.md](survey-prompts.md) for ideas on how to use AI to speed up EDA tasks.

## AI-Assisted Data Visualization

*Placeholder: Demonstrate how to use AI to generate code for creating charts and maps, including interactive visualizations.*

## Interpreting Results with LLMs

*Placeholder: Discuss how to use Large Language Models to help interpret data analysis results and generate narrative summaries.*

## Practical Exercise

*Placeholder: Outline a hands-on exercise where participants will use AI tools to analyze a sample transport dataset.*

```{r}
#| eval: false
#| echo: false
#| label: tfse-data-prep
# Note: the following R code was run once to prepare spatial data files for use in the course.
library(sf)
library(dplyr)

# 1. Read TfSE Boundary (Absolute path as per instruction)
tfse_path = "/home/robin/github/robinlovelace/counterbid/tfse_boundary/TfSE_Area_Boundary.shp"
if(file.exists(tfse_path)){
    tfse_boundary = read_sf(tfse_path)
} else {
    warning("TfSE boundary file not found at specified path.")
}

# 2. Read LADs (Local Authority Districts)
u_lads = "https://services1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/rest/services/Local_Authority_Districts_May_2024_Boundaries_UK_BUC/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson"
lads = read_sf(u_lads)

# 3. Read MSOAs (Middle Layer Super Output Areas) - Centroids for efficiency in spatial join
u_msoas = "https://services1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/rest/services/Middle_Super_Output_Areas_DEC_2021_EW_PWC/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson"
msoas = read_sf(u_msoas)

# 4. Filter Geographies within TfSE
# Ensure CRS match
if(exists("tfse_boundary")){
    tfse_boundary = st_transform(tfse_boundary, st_crs(lads))
    msoas = st_transform(msoas, st_crs(lads))

    # Spatial intersection/subset
    tfse_lads = lads[tfse_boundary, ]
    tfse_msoas = msoas[tfse_boundary, ]

    # 5. Save to GeoPackage
    write_sf(tfse_lads, "tfse_lads.gpkg")
    write_sf(tfse_msoas, "tfse_msoas.gpkg")
    
    print("Files saved: tfse_lads.gpkg, tfse_msoas.gpkg")
}
```

## Advanced Data Acquisition (Optional)

In this section, we demonstrate how to programmatically access and filter large national datasets using R.

```{r}
#| echo: true
#| message: false
#| warning: false
#| eval: true
library(stats19)
library(sf)
library(dplyr)

# 1. Load the study area boundary
# We use the pre-prepared TfSE boundary file
tfse_boundary = read_sf("tfse_boundary.geojson")
```

We can download road safety data directly from the Department for Transport using the `stats19` package.

**Example: Getting 5 years of data (Code only)**

```{r}
#| eval: false
#| echo: true
# This code downloads collision data for 2020-2024
years = 2020:2024
collisions_5y = purrr::map_dfr(years, ~ stats19::get_stats19(year = .x, type = "collision"))
```

**Example: Joining Casualties (Code only)**

```{r}
#| eval: false
#| echo: true
# This code gets casualty data and joins it to the collisions
casualties_5y = purrr::map_dfr(years, ~ stats19::get_stats19(year = .x, type = "casualty"))

casualties_joined = dplyr::left_join(
  casualties_5y,
  collisions_5y,
  by = "collision_index" # Common ID field
)
```

**Live Demo: Getting 2020 Collisions for TfSE**

For this exercise, we will download just the 2020 collision data and filter it to our study area.

```{r}
#| echo: true
#| message: false
#| warning: false

# 1. Get 2020 collision data
collisions_2020 = get_stats19(year = 2020, type = "collision")

# 2. Convert to an 'sf' object (spatial data)
collisions_sf = format_sf(collisions_2020, lonlat = TRUE)

# 3. Filter to the TfSE boundary
# Ensure Coordinate Reference Systems match
collisions_sf = st_transform(collisions_sf, st_crs(tfse_boundary))

# Spatial subset: keep collisions within the boundary
collisions_tfse = collisions_sf[tfse_boundary, ]

print(paste("Original 2020 collisions:", nrow(collisions_sf)))
print(paste("TfSE collisions:", nrow(collisions_tfse)))

# 4. Quick visual check
plot(st_geometry(collisions_tfse), pch = 19, cex = 0.1, col = "red", 
     main = "Road Collisions in TfSE (2020)")
plot(st_geometry(tfse_boundary), add = TRUE, border = "blue")
```
